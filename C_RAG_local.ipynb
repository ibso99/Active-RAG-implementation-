{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aafac562",
   "metadata": {},
   "source": [
    "# Corrective Rag(CRAG) using local LLMS\n",
    "\n",
    "Corrective-RAG (CRAG) is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents.\n",
    "\n",
    "The logic follows this general flow:\n",
    "\n",
    "- If at least one document exceeds the threshold for relevance, then it proceeds to generation\n",
    "- If all documents fall below the relevance threshold or if the grader is unsure, then it uses web search to supplement retrieval\n",
    "Before generation, it performs knowledge refinement of the search or retrieved documents\n",
    "- This partitions the document into knowledge strips\n",
    "It grades each strip, and filters out irrelevant ones\n",
    "We will implement some of these ideas from scratch using LangGraph:\n",
    "\n",
    "- If any documents are irrelevant, we'll supplement retrieval with web search.\n",
    "We'll skip the knowledge refinement, but this can be added back as a node if desired.\n",
    "We'll use Tavily Search for web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f091fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass \n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROK_API_KEY = os.getenv(\"GROK_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "HG_FACE_API_KEY = os.getenv(\"HG_FACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d40ddc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='wizardlm2:latest', modified_at=datetime.datetime(2025, 3, 1, 23, 49, 4, 295170, tzinfo=TzInfo(+03:00)), digest='c9b1aff820f245a43b1719b296ce7131746073691cbb56f7a8d88a4713a3df79', size=4108928625, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='7B', quantization_level='Q4_0')), Model(model='mxbai-embed-large:latest', modified_at=datetime.datetime(2025, 3, 1, 23, 43, 45, 523887, tzinfo=TzInfo(+03:00)), digest='468836162de7f81e041c43663fedbbba921dcea9b9fefea135685a39b2d83dd8', size=669615493, details=ModelDetails(parent_model='', format='gguf', family='bert', families=['bert'], parameter_size='334M', quantization_level='F16')), Model(model='nomic-embed-text:latest', modified_at=datetime.datetime(2025, 2, 25, 15, 1, 57, 368958, tzinfo=TzInfo(+03:00)), digest='0a109f422b47e3a30ba2b10eca18548e944e8a23073ee3f3e947efcf3c45e59f', size=274302450, details=ModelDetails(parent_model='', format='gguf', family='nomic-bert', families=['nomic-bert'], parameter_size='137M', quantization_level='F16')), Model(model='chroma/all-minilm-l6-v2-f32:latest', modified_at=datetime.datetime(2025, 2, 25, 12, 26, 44, 235140, tzinfo=TzInfo(+03:00)), digest='6c031a734080a617b5edfa99adaa9fc7424c4b7f3ae2099bdac710abb3f54ee6', size=91028860, details=ModelDetails(parent_model='', format='gguf', family='bert', families=['bert'], parameter_size='23M', quantization_level='F32')), Model(model='qwen2.5:latest', modified_at=datetime.datetime(2025, 2, 12, 10, 28, 37, 212036, tzinfo=TzInfo(+03:00)), digest='845dbda0ea48ed749caafd9e6037047aa19acfcfd82e704d7ca97d631a0b697e', size=4683087332, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='7.6B', quantization_level='Q4_K_M')), Model(model='deepseek-r1:7b', modified_at=datetime.datetime(2025, 2, 11, 15, 48, 2, 645805, tzinfo=TzInfo(+03:00)), digest='0a8c266910232fd3291e71e5ba1e058cc5af9d411192cf88b6d30e92b6e73163', size=4683075271, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='7.6B', quantization_level='Q4_K_M')), Model(model='llama3.2:latest', modified_at=datetime.datetime(2025, 2, 6, 12, 19, 38, 28978, tzinfo=TzInfo(+03:00)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M'))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama \n",
    "available_models = ollama.list()\n",
    "available_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "711adcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_llm = \"llama3.2:latest\"\n",
    "local_llm = \"qwen2.5:latest\"\n",
    "model_tested = \"llama3.2:latest\"\n",
    "metadata = f\"CRAG, {model_tested}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad78f5da",
   "metadata": {},
   "source": [
    "# Creating Index or Vector store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efe5690d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3\n",
      "Number of splits: 194\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore, Chroma\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "\n",
    "# URL list to scrape\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# load documents from url \n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_lists = [item for sublist in docs for item in sublist] # Basically flatten the list\n",
    "\n",
    "# Initialize a text splitter with specified chunksize and overlap \n",
    "\n",
    "test_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 250,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "\n",
    "# Split the documents into smaller chunks \n",
    "doc_splits = test_splitter.split_documents(docs_lists)\n",
    "print(f\"Number of documents: {len(docs_lists)}\")\n",
    "print(f\"Number of splits: {len(doc_splits)}\")\n",
    "# print(f\"First split: {doc_splits[0].page_content}\")\n",
    "# print(f\"First document: {docs_lists[0].page_content}\")\n",
    "\n",
    "# Embeddings \n",
    "\n",
    "embedding = OllamaEmbeddings(model=\"chroma/all-minilm-l6-v2-f32:latest\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = doc_splits,\n",
    "    collection_name = \"rag_chroma\",\n",
    "    embedding = embedding\n",
    ")\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever(  search_type=\"mmr\",\n",
    "        search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d54029a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'language': 'en'}, page_content='Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'), Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content=\"Planning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\"), Document(metadata={'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}, page_content='Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Fig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.')]\n"
     ]
    }
   ],
   "source": [
    "question = \"agent memory\"\n",
    "\n",
    "docs = retriever.invoke(question)\n",
    "# print(docs[])\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31b99db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 1}\n"
     ]
    }
   ],
   "source": [
    "# Define Tools\n",
    "### Retrieval Grader \n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt \n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a teacher grading a quiz. You will be given: \n",
    "    1/ a QUESTION\n",
    "    2/ A FACT provided by the student\n",
    "    \n",
    "    You are grading RELEVANCE RECALL:\n",
    "    A score of 1 means that ANY of the statements in the FACT are relevant to the QUESTION. \n",
    "    A score of 0 means that NONE of the statements in the FACT are relevant to the QUESTION. \n",
    "    1 is the highest (best) score. 0 is the lowest score you can give. \n",
    "    \n",
    "    Explain your reasoning in a step-by-step manner. Ensure your reasoning and conclusion are correct. \n",
    "    \n",
    "    Avoid simply stating the correct answer at the outset.\n",
    "    \n",
    "    Question: {question} \\n\n",
    "    Fact: \\n\\n {documents} \\n\\n\n",
    "    \n",
    "    Give a binary score '1' or '0' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"agent memory\"\n",
    "\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "print(retrieval_grader.invoke({\"question\": question, \"documents\": docs}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7915247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided, it seems you are discussing an LLM-powered autonomous agent system and its components, particularly focusing on planning, memory, and tool use.\n",
      "\n",
      "### Planning:\n",
      "- **Subgoal Decomposition**: The agent breaks down complex tasks into smaller, manageable subgoals to handle complexity efficiently.\n",
      "- **Self-Criticism and Reflection**: The agent can evaluate past actions, learn from mistakes, and refine future steps to improve overall performance.\n",
      "\n",
      "### Memory:\n",
      "- **Short-Term Memory (ST):** Utilizes in-context learning or prompt engineering techniques where the model learns from immediate inputs and context.\n",
      "- **Long-Term Memory (LT):** Employs external vector stores for retaining and retrieving vast amounts of information over extended periods, enabling the agent to recall historical data and knowledge.\n",
      "\n",
      "### Tool Use:\n",
      "- The agent can access external APIs to gather necessary information not available in its pre-trained model weights. This includes current data, code execution capabilities, and proprietary information sources.\n",
      "\n",
      "### Additional Insights from Provided Content:\n",
      "1. **CoH (Continual Human Feedback):** Fine-tuning the LLM with a history of sequentially improved outputs helps the model produce better results incrementally.\n",
      "2. **Algorithm Distillation (AD) in Reinforcement Learning:** This technique concatenates learning histories to improve future predictions, aiming to learn the process of reinforcement learning rather than training task-specific policies.\n",
      "\n",
      "### Summary:\n",
      "The system leverages advanced techniques like continual human feedback and algorithm distillation to enhance its performance over time while utilizing external tools and memory systems for comprehensive data handling.\n"
     ]
    }
   ],
   "source": [
    "### Generate \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt \n",
    "prompt = PromptTemplate(\n",
    "    template = \"\"\"You are an assistant for question-answering tasks. \n",
    "    \n",
    "    Use the following documents to answer the question. \n",
    "    \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    \n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Documents: {documents} \n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")\n",
    "\n",
    "# LLM \n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Chain \n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Runnning \n",
    "\n",
    "generation = rag_chain.invoke({\"documents\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfcd3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
