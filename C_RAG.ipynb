{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d3ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass \n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROK_API_KEY = os.getenv(\"GROK_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "HG_FACE_API_KEY = os.getenv(\"HG_FACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc932e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader \n",
    "from langchain_community.vectorstores import Chroma \n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "urls = [\n",
    "       \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# print(\"whats up\")\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Adding to vector database \n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "#                                                api_key=HG_FACE_API_KEY)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "                                  # api_key=HG_FACE_API_KEY)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = doc_splits,\n",
    "    collection_name = \"rag_chroma\",\n",
    "    embedding = embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b240442-7a12-422e-9421-47e82416e3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# LLM\n",
    "# Retrievr Grader \n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq \n",
    "\n",
    "# Data model \n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\" Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        decription = \"Document are relevant to the question, 'yes' or 'no' \"\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOllama(\n",
    "    model = \"llama3.2:latest\",\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256,\n",
    ")\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fff436e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text does not provide a specific question or problem to be solved. It appears to be a passage from an article or blog post about Large Language Models (LLMs) and their potential use in autonomous agent systems. The passage discusses the challenges and limitations of using LLMs in these systems, including finite context length, challenges in long-term planning and task decomposition, and reliability of natural language interfaces.\n",
      "\n",
      "If you could provide a specific question or problem related to LLMs or autonomous agent systems, I would be happy to try and assist you.\n"
     ]
    }
   ],
   "source": [
    "###Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "llm = ChatOllama(\n",
    "     model = \"llama3.2:latest\",\n",
    "    temperature = 0,\n",
    "    num_predict = 256,\n",
    ")\n",
    "\n",
    "# Post-processing \n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain \n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run \n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c7aea38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After analyzing the input question, I\\'ve rewritten it to better capture the intended meaning and optimize it for web search:\\n\\n**Improved Question:** \"What types of digital or neural memory systems are used in AI agents?\"\\n\\nThis revised question is more specific, concise, and relevant to search engines\\' algorithms. It also clearly conveys the user\\'s intent to learn about artificial intelligence (AI) agent memory systems.\\n\\nReasoning behind the improvement:\\n\\n1. **Specificity**: The original question was quite broad, using the term \"agent memory\" without specifying what type of agents or memory systems were being referred to. The revised question adds context by mentioning \"digital or neural memory systems,\" which narrows down the scope.\\n2. **Clarity**: The revised question is easier to understand, as it clearly states the user\\'s intent to learn about AI agent memory systems. This clarity helps search engines accurately interpret the query and return relevant results.\\n3. **Relevance**: The revised question is more likely to return accurate and relevant results from web searches, as it targets specific information related to AI agents and their memory systems.\\n\\nWould you like me to rewrite the question further or refine any aspect of this improved version?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question Re-Writing \n",
    "llm = ChatOllama(\n",
    "    model = \"llama3.2:latest\",\n",
    "    temprature = 0,   \n",
    ")\n",
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewritter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewritter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92034e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web search tool \n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3, TAVILY_API_KEY=TAVILY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1a91e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph \n",
    "# Defining Graph state \n",
    "\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "        \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "        question : str\n",
    "        generation: str\n",
    "        web_search: str\n",
    "        documents: List[str]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b0d697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVER---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    #   Retrieval \n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print( \"---GENERATION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \n",
    "            \"question\": question,\n",
    "            \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc \n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            web_search = \"yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewritter.invoke({\"question\": question})\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": better_question\n",
    "    }\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search \n",
    "    docs = web_search_tool.invoke(\n",
    "        {\"query\": question}\n",
    "    )\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documnents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "#### Edges \n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "         # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eaca2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Graph \n",
    "\n",
    "from langgraph.graph import END, StateGraph, START \n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define nodes\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve) \n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"web_search_node\", web_search)\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\", \n",
    "    decide_to_generate,\n",
    "                {\n",
    "                     \"transform_query\": \"transform_query\", \n",
    "                     \"generate\": \"generate\",\n",
    "                 },\n",
    "            )\n",
    "\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "\n",
    "# Compile \n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e87b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVER---\n",
      "\"Node  'retrieve': \"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node  'grade_documents': \"\n",
      "'\\n---\\n'\n",
      "---GENERATION---\n",
      "\"Node  'generate': \"\n",
      "'\\n---\\n'\n",
      "(\"I don't have any retrieved context provided for this question. Please \"\n",
      " 'provide more information or clarify which type of \"agents\" you\\'re referring '\n",
      " \"to (e.g., law enforcement, medical, etc.). I'll do my best to assist you.\")\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run \n",
    "inputs = { \"question\": \"what arev the types of agents?\" }\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node \n",
    "        pprint(f\"Node  '{key}': \")\n",
    "         # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a7c7dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVER---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "(\"I don't know the specific details about how the AlphaCodium paper works, as \"\n",
      " 'it is not mentioned in the provided context. The context discusses '\n",
      " 'adversarial attacks on large language models (LLMs) and their challenges, '\n",
      " 'but does not mention the AlphaCodium paper specifically. If you have more '\n",
      " 'information or context about the AlphaCodium paper, I may be able to help '\n",
      " 'further.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": \"How does the AlphaCodium paper work?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4261ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
